{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNxf-LwzPz1y"
      },
      "source": [
        "Prepare colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R7Q0VrdN6DV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets>=1.18.3\n",
        "!pip install transformers==4.11.3\n",
        "!pip install librosa\n",
        "!pip install jiwer\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1QrTp575yLmF"
      },
      "outputs": [],
      "source": [
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8Ya1yL6Ca7X0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from jiwer import wer\n",
        "\n",
        "\n",
        "librispeech_eval = load_dataset('librispeech_test_clean', split=\"test-clean\")\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(\"cuda\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wK2RQpK9nwTG"
      },
      "outputs": [],
      "source": [
        "def map_to_pred(batch):\n",
        "    input_values = processor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "    batch[\"transcription\"] = transcription[0].lower()\n",
        "    batch[\"sentence\"] = batch[\"sentence\"].lower()\n",
        "    return batch\n",
        "\n",
        "result = librispeech_eval.map(map_to_pred)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkve-PWsxchl",
        "outputId": "fe1ac53d-3921-4684-a237-7274ec6736ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER: 0.02765520389531345\n"
          ]
        }
      ],
      "source": [
        "print(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE190poYyOCx",
        "outputId": "dc943fa0-4232-48ad-ff8d-c06b1ff15084"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU: {'bleu': 0.9435228921373371, 'precisions': [0.9746081862446744, 0.9531988149571623, 0.9333093920317715, 0.9140513233190272], 'brevity_penalty': 1.0, 'length_ratio': 1.0000570635116885, 'translation_length': 52576, 'reference_length': 52573}\n",
            "ROUGE: {'rouge1': 0.9712394139771212, 'rouge2': 0.9466495598030624, 'rougeL': 0.9711493254688222, 'rougeLsum': 0.9711588997891094}\n"
          ]
        }
      ],
      "source": [
        "bleu = evaluate.load('bleu')\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "bleu_res = bleu.compute(predictions=result[\"text\"], references=result[\"transcription\"])\n",
        "rouge_res = rouge.compute(predictions=result[\"text\"], references=result[\"transcription\"])\n",
        "\n",
        "print(f\"BLEU: {bleu_res}\\nROUGE: {rouge_res}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}