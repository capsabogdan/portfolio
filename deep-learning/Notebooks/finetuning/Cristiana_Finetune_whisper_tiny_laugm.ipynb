{"cells":[{"cell_type":"markdown","source":["This notebook is modifed from Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers Colab created by Sanchit Gandhi avalible at https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb. Our work is only the modifications to the original notebook."],"metadata":{"id":"vEEHOul0uwev"}},{"cell_type":"markdown","metadata":{"id":"ekhirbjKtjPW"},"source":["# GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAN1t_-mtkHj","outputId":"2632d28b-2d6c-4f52-d412-523ef77cc6f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Dec  5 06:58:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    21W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"lIpQY5QWtm7M"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baOJduxttpgD","outputId":"9c0a9b47-f723-4913-ef2d-08f894963109"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-uzweyubl\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-uzweyubl\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}],"source":["# use datasets to download and prepare our training data and transformers to load and train our Whisper model.\n","!pip install datasets>=2.6.1\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install librosa\n","!pip install evaluate>=0.30\n","!pip install jiwer\n","!pip install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajA1XvXyuJXV"},"outputs":[],"source":["\n","token = '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'\n","\n","# import the relavant libraries for loggin in\n","from huggingface_hub import HfApi, HfFolder\n","\n","# set api for login and save token\\\n","api=HfApi()\n","api.set_access_token(token)\n","folder = HfFolder()\n","folder.save_token(token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCSymdGdg4Wc"},"outputs":[],"source":["import pickle\n","from datasets import Audio\n","from datasets import Dataset\n","from datasets import Features"]},{"cell_type":"markdown","metadata":{"id":"SW13aT4Xg465"},"source":["# Load Data"]},{"cell_type":"markdown","metadata":{"id":"BYV8PW1VwA9d"},"source":[" ## Load WhisperFeatureExtractor\n"," load feature extractor from the pre-trained checkpoint with default values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGNKZe1iuwBq"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny.en\")"]},{"cell_type":"markdown","metadata":{"id":"VNcwVEBov_k7"},"source":["## Load WhisperTokenizer\n","\n","Whisper model outputs a sequence of token ids. \n","\n","The tokenizer maps each of these token ids to their corresponding text string. \n","\n","We will load the pre-trained tokenizer and use it for fine-tuning without any further modifications."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCc6dlTqwOxD"},"outputs":[],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\", language=\"English\", task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{"id":"2IHJcl14wucQ"},"source":["## Combine To Create A WhisperProcessor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEQU-yRPwgVC"},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\", language=\"English\", task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{"id":"lHT1kt1d6NUK"},"source":["## Load DataSet from Hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0_zZvNPZGUSO"},"outputs":[],"source":["from datasets import load_dataset\n","from datasets import DownloadConfig\n","\n","laugm_train = load_dataset(r\"DTU54DL/librispeech5k-augmentated-train-prepared\", download_config=DownloadConfig(delete_extracted=True))\n","laugm_val = load_dataset(r\"DTU54DL/librispeech-augmentated-validation-prepared\", download_config=DownloadConfig(delete_extracted=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDpFynDHA9m5"},"outputs":[],"source":["train = laugm_train[\"train.360\"].select(range(0, 500))\n","test = laugm_val[\"validation\"].select(range(0, 100))"]},{"cell_type":"markdown","metadata":{"id":"gnF5hZzHpca0"},"source":["# Feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUTOv0ETpk91"},"outputs":[],"source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array \n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features[0]\n","\n","    # encode target text to label ids \n","    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qx7W1kn3ppCB"},"outputs":[],"source":["train = train.map(prepare_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NearHU6cpvse"},"outputs":[],"source":["test = test.map(prepare_dataset)"]},{"cell_type":"markdown","metadata":{"id":"IfxKnkl2w4DL"},"source":["# Training and Evaluation\n","\n","We'll follow these steps:\n","\n","* Define **data collator**: data collator takes pre-processed data and prepares PyTorch tensors ready for the model.\n","* **Evaluation metrics**: during evaluation, we evaluate the model using WER metric. We need to define a compute_metrics function that handles this computation.\n","* **Load pre-trained checkpoint**: load a pre-trained checkpoint and configure it correctly for training\n","* Define **training configuration**: this will be used by **Trainer** to define the training schedule.\n","\n","After tuning the model, we evaluate it on test data to verify that we have correctly trained it to transcribe speech."]},{"cell_type":"markdown","metadata":{"id":"rhvSDgTdw_vJ"},"source":["## Define Data Collator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5JAUJQ9w2ZX"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"markdown","metadata":{"id":"6t5qJtOfyGIp"},"source":["Initialise the defined data collator :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Fxff-yTyCx2"},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"jn37u8hNyO-H"},"source":["## Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SYpwwcpyR6e"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"markdown","metadata":{"id":"BKd6ApmWyb4k"},"source":["Define function that takes model predictions and returns the WER metric.\n","\n","* It first replaces -100 with the pad_token_id in the label_ids (undoing the step we applied in the data collator to ignore padded tokens correctly in the loss).\n","\n","* It then decodes the predicted and label ids to strings. \n","\n","* Finally, it computes the WER between the predictions and reference labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7fYq5nxytdW"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","metadata":{"id":"xciYcqZEzJQE"},"source":["## Define Training Configuration\n","\n","**Final step**: define all parameters related to training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54PCQUDCzKrv"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/whisper-tiny-laugm\",  # change to a repo name of your choice\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"acyAvX1wzYbE"},"source":["**Note**: if one does not want to upload the model checkpoints to the Hub, set push_to_hub=False."]},{"cell_type":"markdown","metadata":{"id":"2TBLOACNzcui"},"source":["Forward training arguments to Trainer along with model,\n","dataset, data collator and `compute_metrics` function"]},{"cell_type":"markdown","metadata":{"id":"HBhdTmYUyyaJ"},"source":["## Load a Pre-Trained Checkpoint "]},{"cell_type":"markdown","metadata":{"id":"U4_xt20FzGWg"},"source":["Override generation arguments - no tokens are forced as decoder outputs (see forced_decoder_ids), no tokens are suppressed during generation (see suppress_tokens):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbucuiXHyvO2"},"outputs":[],"source":["# load the pre-trained Whisper small checkpoint.\n","from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC-ZtgAFy5JQ"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsB-a8w4zV7Z"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=train,\n","    eval_dataset=test,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n","    \n",")"]},{"cell_type":"markdown","metadata":{"id":"wN5Yx9lRzoVS"},"source":["## Training\n","\n","Training will take approx 5-10 hours depending on GPU / the one allocated to this Google Colab. If using this Google Colab directly to fine-tune a Whisper model, you should make sure that training isn't interrupted due to inactivity. \n","\n","Simple workaround to prevent this is to paste the following code into the console of this tab (right mouse click -> inspect -> Console tab -> insert code)."]},{"cell_type":"markdown","metadata":{"id":"PZkTXoHS0Ndw"},"source":["```javascript\n","function ConnectButton(){\n","    console.log(\"Connect pushed\"); \n","    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n","}\n","setInterval(ConnectButton, 60000);\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"marWuVn4zzPl"},"outputs":[],"source":["trainer.train()\n","torch.cuda.empty_cache() "]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}